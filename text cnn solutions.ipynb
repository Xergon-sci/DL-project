{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-16 09:59:18--  https://raw.githubusercontent.com/cezannec/CNN_Text_Classification/master/data/reviews.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33678267 (32M) [text/plain]\n",
      "Saving to: ‘data/reviews.txt’\n",
      "\n",
      "reviews.txt         100%[===================>]  32.12M  5.77MB/s    in 5.6s    \n",
      "\n",
      "2021-11-16 09:59:25 (5.74 MB/s) - ‘data/reviews.txt’ saved [33678267/33678267]\n",
      "\n",
      "--2021-11-16 09:59:26--  https://raw.githubusercontent.com/cezannec/CNN_Text_Classification/master/data/labels.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 225000 (220K) [text/plain]\n",
      "Saving to: ‘data/labels.txt’\n",
      "\n",
      "labels.txt          100%[===================>] 219.73K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2021-11-16 09:59:26 (5.41 MB/s) - ‘data/labels.txt’ saved [225000/225000]\n",
      "\n",
      "--2021-11-16 09:59:27--  https://github.com/bekou/multihead_joint_entity_relation_extraction/raw/master/data/CoNLL04/vecs.lc.over100freq.zip\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/bekou/multihead_joint_entity_relation_extraction/master/data/CoNLL04/vecs.lc.over100freq.zip [following]\n",
      "--2021-11-16 09:59:27--  https://raw.githubusercontent.com/bekou/multihead_joint_entity_relation_extraction/master/data/CoNLL04/vecs.lc.over100freq.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 80318758 (77M) [application/zip]\n",
      "Saving to: ‘data/vecs.lc.over100freq.zip’\n",
      "\n",
      "vecs.lc.over100freq 100%[===================>]  76.60M  7.37MB/s    in 12s     \n",
      "\n",
      "2021-11-16 09:59:42 (6.60 MB/s) - ‘data/vecs.lc.over100freq.zip’ saved [80318758/80318758]\n",
      "\n",
      "Archive:  data/vecs.lc.over100freq.zip\n",
      "  inflating: data/vecs.lc.over100freq.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/cezannec/CNN_Text_Classification/master/data/reviews.txt -P data/\n",
    "!wget https://raw.githubusercontent.com/cezannec/CNN_Text_Classification/master/data/labels.txt -P data/\n",
    "!wget https://github.com/bekou/multihead_joint_entity_relation_extraction/raw/master/data/CoNLL04/vecs.lc.over100freq.zip -P data/\n",
    "!unzip data/vecs.lc.over100freq.zip -d data/    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print the first 1000 characters from the text--note this is not yet \n",
    "ready for classification, we should split it to sentences first\n",
    "\"\"\"\n",
    "print(reviews[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print the first 20 characters from the text--note this is not yet \n",
    "ready for classification, we should split it to sentences first\n",
    "\"\"\"\n",
    "labels[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\"\"\"First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words.\"\"\"\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of all words\n",
    "all_words = all_text.split() # this is the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_split[0] # this is what the data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews_split) # this is the size of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Labels\n",
    "\n",
    "The review labels are \"positive\" or \"negative\". \n",
    "Split them with the newline character to have the same size as reviews split and then transform them into 0 or 1 to feed them to the neural network. 1 in the case that the label is positive, 0 in the case that it is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1=positive, 0=negative label conversion\n",
    "labels_split = labels.split('\\n')\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Build a dictionary that maps indices to review lengths\n",
    "counts = Counter(all_words)\n",
    "# outlier review stats\n",
    "# counting words in each review\n",
    "review_lens = Counter([len(x.split()) for x in reviews_split])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of extremely long or short reviews; the outliers\n",
    "\n",
    "print('Number of reviews before removing outliers: ', len(reviews_split))\n",
    "\n",
    "## remove any reviews/labels with zero length from the reviews_ints list.\n",
    "\n",
    "# get indices of any reviews with length 0\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_split) if len(review.split()) != 0]\n",
    "\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_split = [reviews_split[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Pre-Trained Embedding Layer\n",
    "\n",
    "Next, we will tokenize the reviews; turn list of words that make up a given review into a list of tokenized integers that represent those words. Typically, this is done by creating a dictionary that maps each unique word in a vocabulary to a specific integer value.\n",
    "\n",
    "In this example, I'll use a mapping that already exists, in a pre-trained embedding layer. Below, I am loading a pre-trained embeddings matrix.\n",
    "\n",
    "You can download the matrix and explore it your self (from the link at the beginning of the exercise). It contains 412128 tokens with the corresponding pretrained vector for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Word2Vec loading capabilities\n",
    "from gensim.models import KeyedVectors\n",
    "# Creating the model\n",
    "embed_lookup = KeyedVectors.load_word2vec_format('data/vecs.lc.over100freq.txt', \n",
    "                                                 binary=False,unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"You can think of an embedding layer as a lookup table, where the rows are indexed by word token and the \n",
    "columns hold the embedding values. For example, row 4 is the embedding vector for the word that maps to the integer value 4.\n",
    "In the cells below, we are storing the words in the pre-trained vocabulary, and printing out the size of the \n",
    "vocabulary and word embeddings. The embedding dimension from the pre-trained model is 50.\"\"\"\n",
    "\n",
    "# store pretrained vocab\n",
    "pretrained_words = []\n",
    "for word in embed_lookup.vocab:\n",
    "    pretrained_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx = 4\n",
    "\n",
    "# get word/embedding in that row\n",
    "word = pretrained_words[row_idx] # get words by index\n",
    "embedding = embed_lookup[word] # embeddings by word\n",
    "\n",
    "# vocab and embedding info\n",
    "print(\"Size of Vocab: {}\\n\".format(len(pretrained_words)))\n",
    "print('Word in vocab: {}\\n'.format(word))\n",
    "print('Length of embedding: {}\\n'.format(len(embedding)))\n",
    "print('Associated embedding: \\n', embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a few common words\n",
    "for i in range(3,8):\n",
    "    print(pretrained_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize reviews\n",
    "\n",
    "The pre-trained embedding layer already has tokens associated with each word in the dictionary. We want to use that same mapping to tokenize all the reviews in the movie review corpus. We will encode any unknown words (words that appear in the reviews but not in the pre-trained vocabulary) as the <unk> token, 9; This appears in the 9th position of the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reviews to tokens\n",
    "def tokenize_all_reviews(embed_lookup, reviews_split):\n",
    "    # split each review into a list of words\n",
    "    reviews_words = [review.split() for review in reviews_split]\n",
    "\n",
    "    tokenized_reviews = []\n",
    "    for review in reviews_words:\n",
    "        ints = []\n",
    "        for word in review:\n",
    "            try:\n",
    "                idx = embed_lookup.vocab[word].index\n",
    "            except: \n",
    "                idx = 9\n",
    "                \n",
    "            ints.append(idx)\n",
    "        tokenized_reviews.append(ints)\n",
    "    \n",
    "    return tokenized_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = tokenize_all_reviews(embed_lookup, reviews_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing code and printing a tokenized review for the first document\n",
    "print(tokenized_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Padding sequences\n",
    "\n",
    "To deal with both short and very long reviews, we should have the same length for all reviews in the neural network\n",
    "For that, we will pad all reviews shorter than a specific length with a number. For us, it will be 1 because this is \n",
    "the PADDING id in the embeddings matrix (validate that in the vocabulary). \n",
    "For all reviews that are longer than this specific length, we will cut them.\n",
    "\n",
    "\n",
    "As a small example, if the `seq_length=10` and an input, tokenized review is: \n",
    "```\n",
    "[117, 18, 128]\n",
    "```\n",
    "The resultant, padded sequence should be: \n",
    "\n",
    "```\n",
    "[1, 1, 1, 1 1, 1, 1, 117, 18, 128]\n",
    "```\n",
    "\n",
    "**Your final `features` array should be a 2D array, with as many rows as there are reviews, and as many columns as the specified `seq_length`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(tokenized_reviews, seq_length):\n",
    "    ''' Return features of tokenized_reviews, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(tokenized_reviews), seq_length), dtype=int)+1\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(tokenized_reviews):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation!\n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(tokenized_reviews, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(tokenized_reviews), \"Features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training, Validation, and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#This part is similar to what we have done in the previous exercises\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# shuffling and batching data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Network with PyTorch\n",
    "\n",
    "The complete model consists of a few layers:\n",
    "\n",
    "**1. An [embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)**\n",
    "* This converts our word tokens (integers) into embedded vectors of a specific size.\n",
    "* In this case, the vectors/weights of this layer will come from the **pretrained** lookup table defined above. \n",
    "\n",
    "**2. A few [convolutional layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)**\n",
    "* These are defined by an input size, number of filters/feature maps to output, and a kernel size.\n",
    "* The output of these layers will go through a ReLu activation function and pooling layer in the `forward` function.\n",
    "\n",
    "**3. A fully-connected, output layer**\n",
    "* This maps the convolutional layer outputs to a desired output_size (1 sentiment class).\n",
    "\n",
    "**4. A sigmoid activation layer**\n",
    "* This turns the output logit into a value 0-1; a class score.\n",
    "\n",
    "There is also a dropout layer, which will prevent overfitting, placed between the convolutional outputs and the final, fully-connected layer.\n",
    "\n",
    "See the original paper, [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf).\n",
    "\n",
    "### The Embedding Layer\n",
    "\n",
    "The embedding layer comes from our pre-trained `embed_lookup` model. By default, the weights of this layer are set to the vectors from the pre-trained model and frozen, so it will just be used as a lookup table. You could train your own embedding layer here, but it will speed up the training process to use a pre-trained model.\n",
    "\n",
    "### The Convolutional Layer(s)\n",
    "\n",
    "So why use CNNs on text? In the same way that a 3x3 filter can look over a patch of an image, a 1x2 filter can look over a 2 sequential words in a piece of text, i.e. a bi-gram. In this CNN model we will use multiple filters of different sizes which will look at the bi-grams (two words, a 1x2 filter), tri-grams (three words, a 1x3 filter) and/or n-grams (a 1x$n$ filter) within the text.\n",
    "\n",
    "The intuition here is that the appearance of certain bi-grams, tri-grams and n-grams within the review will be a good indication of the final sentiment.\n",
    "\n",
    "We can then use a filter that is [n x emb_dim]. This will cover $n$ sequential words entirely, as their width will be emb_dim dimensions.\n",
    "\n",
    "The kernel_sizes would (3, 50), (4, 50), and (5, 50); to look at 3-, 4-, and 5- sequences of word embeddings at a time (50 is the size of the word embeddings). Each of these three layers will produce 100 filtered outputs, where 100 is the number of filters. \n",
    "\n",
    "The kernels only move in one dimension: down to a sequence of word embeddings. In other words, these kernels move along a sequence of words, in time!\n",
    "\n",
    "### Maxpooling Layers\n",
    "\n",
    "In the `forward` function, we apply a ReLu activation to the outputs of all convolutional layers and a maxpooling layer over the input sequence dimension. The maxpooling layer will get us an indication of whether some high-level text feature was found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding layer + CNN model that will be used to perform sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_model, vocab_size, output_size, embedding_dim,\n",
    "                 num_filters=100, kernel_sizes=[3, 4, 5], freeze_embeddings=True, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentCNN, self).__init__()\n",
    "\n",
    "        # set class vars\n",
    "        self.num_filters = num_filters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 1. embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # set weights to pre-trained\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n",
    "        # (optional) freeze embedding weights\n",
    "        if freeze_embeddings:\n",
    "            self.embedding.requires_grad = False\n",
    "        \n",
    "        # 2. convolutional layers             \n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = num_filters, \n",
    "                                kernel_size = (kernel_sizes[0], embedding_dim))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = num_filters, \n",
    "                                kernel_size = (kernel_sizes[1], embedding_dim))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = num_filters, \n",
    "                                kernel_size = (kernel_sizes[2], embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines how a batch of inputs, x, passes through the model layers.\n",
    "        Returns a single, sigmoid-activated class score as output.\n",
    "        \"\"\"\n",
    "        # embedded vectors\n",
    "        embeds = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n",
    "        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n",
    "        embeds = embeds.unsqueeze(1)\n",
    "                \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embeds).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embeds).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embeds).squeeze(3))\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))    \n",
    "        \n",
    "        logit = self.fc(cat)\n",
    "        \n",
    "        # sigmoid-activated --> a class score\n",
    "        return self.sig(logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, I'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `num_filters`: Number of filters that each convolutional layer produces as output.\n",
    "* `filter_sizes`: A list of kernel sizes; one convolutional layer will be created for each kernel size.\n",
    "\n",
    "Any parameters I did not list, are left as the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "\n",
    "vocab_size = len(pretrained_words)\n",
    "output_size = 1 # binary class (1 or 0)\n",
    "embedding_dim = len(embed_lookup[pretrained_words[0]]) # 50-dim vectors\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "net = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "                   num_filters, kernel_sizes)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(net, train_loader, epochs, print_every=100):\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    counter = 0 # for printing\n",
    "    \n",
    "    # train for some number of epochs\n",
    "    net.train()\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output = net(inputs)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output = net(inputs)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 2 # this is approx where I noticed the validation loss stop decreasing\n",
    "print_every = 100\n",
    "\n",
    "train(net, train_loader, epochs, print_every=print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output = net(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
